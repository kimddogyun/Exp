{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e36e1f",
   "metadata": {},
   "source": [
    "# 5. 인공지능과 가위바위보 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205ac523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eea9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toltal : 2889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2889"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train set resize\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rsp_01/rps\"\n",
    "\n",
    "def resize_images(img_path):\n",
    "    classes = ['/scissors', '/paper', '/rock']\n",
    "    target_size = (32,32)\n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    for c in classes :\n",
    "        images=glob.glob(img_path + c + \"/*.jpg\")\n",
    "        for img in images:\n",
    "            old_img=Image.open(img)\n",
    "            new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "            new_img.save(img, \"JPEG\")\n",
    "            num += 1\n",
    "        \n",
    "    print('Toltal :',num)\n",
    "    \n",
    "    return num\n",
    "\n",
    "numbumer_of_data = resize_images(image_dir_path)\n",
    "\n",
    "numbumer_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb927ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toltal : 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test셋에 대해서도 마찬가지로 Resize\n",
    "\n",
    "image_test_dir_path = os.getenv(\"HOME\") + \"/aiffel/DATA_RSP/test\"\n",
    "resize_images(image_test_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4761788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 2889 입니다.\n",
      "x_train shape: (2889, 32, 32, 3)\n",
      "y_train shape: (2889,)\n"
     ]
    }
   ],
   "source": [
    "#train data에 대해 3개의 클래스 즉, 가위: 0, 바위: 1, 보: 2 로 라벨링\n",
    "\n",
    "def load_data(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=2889\n",
    "    img_size=32\n",
    "    color=3\n",
    "    \n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(train_dir+'/scissors/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(train_dir+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(train_dir+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "train_dir = os.getenv(\"HOME\") + \"/aiffel/rsp_01/rps\"\n",
    "(x_train, y_train)=load_data(train_dir)\n",
    "\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558fb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터(x_test)의 이미지 개수는 200 입니다.\n",
      "x_test shape: (300, 32, 32, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "#test data에 대해 3개의 클래스 즉, 가위: 0, 바위: 1, 보: 2 로 라벨링\n",
    "def load_data(ima_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=300 # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size=32\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(test_dir+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(test_dir+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(test_dir+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "test_dir = os.getenv(\"HOME\") + \"/aiffel/DATA_RSP/test\"\n",
    "(x_test, y_test)=load_data(test_dir)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "558ce942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5322ac1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1])\n",
    "print('라벨:', y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c37a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습을 위한 channel, dense, img_size 지정\n",
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_channel_3 = 128\n",
    "\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd11cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#채널 추가를 위한 data reshape\n",
    "x_train_reshaped=x_train_norm.reshape( -1, 32, 32, 3)  \n",
    "x_test_reshaped=x_test_norm.reshape( -1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47e9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 6915      \n",
      "=================================================================\n",
      "Total params: 26,307\n",
      "Trainable params: 26,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Baseline 모델 만들어보기!\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0e05c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/keras/backend.py:4906: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 5s 165ms/step - loss: 0.9815 - accuracy: 0.5760 - val_loss: 1.1857 - val_accuracy: 0.0267\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 5s 158ms/step - loss: 0.5550 - accuracy: 0.8598 - val_loss: 1.4725 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.2516 - accuracy: 0.9408 - val_loss: 1.5207 - val_accuracy: 0.0067\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 5s 163ms/step - loss: 0.1531 - accuracy: 0.9585 - val_loss: 1.6531 - val_accuracy: 0.0067\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 5s 160ms/step - loss: 0.0954 - accuracy: 0.9848 - val_loss: 1.3278 - val_accuracy: 0.1767\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 4s 155ms/step - loss: 0.0657 - accuracy: 0.9896 - val_loss: 1.5203 - val_accuracy: 0.1233\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0444 - accuracy: 0.9945 - val_loss: 1.6987 - val_accuracy: 0.1100\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 4s 154ms/step - loss: 0.0363 - accuracy: 0.9962 - val_loss: 1.6798 - val_accuracy: 0.1367\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0287 - accuracy: 0.9958 - val_loss: 1.6671 - val_accuracy: 0.2733\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0277 - accuracy: 0.9965 - val_loss: 1.8139 - val_accuracy: 0.1100\n",
      "10/10 - 0s - loss: 1.8139 - accuracy: 0.1100\n",
      "test_loss: 1.8139355182647705 \n",
      "test_accuracy: 0.10999999940395355\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "history = model.fit(x_train_reshaped, y_train, batch_size=100,epochs=10, \n",
    "                    verbose=1, validation_data=(x_test_reshaped, y_test),\n",
    "                    shuffle=True, callbacks =[callback])\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5e475",
   "metadata": {},
   "source": [
    "test_accuracy가 0.109로 형편없는 것을 확인. 다르게 모델링 해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f932574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#층을 좀 더 깊이 쌓고, Overfitting 방지를 위해 Dropout 추가 및 정규화를 위한 he_uniform 적용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea6a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 357,443\n",
      "Trainable params: 357,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', \n",
    "                              input_shape=(IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "model.add(keras.layers.MaxPooling2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a644b5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "91/91 [==============================] - 6s 65ms/step - loss: 0.5856 - accuracy: 0.7459 - val_loss: 1.4375 - val_accuracy: 0.1300\n",
      "Epoch 2/10\n",
      "91/91 [==============================] - 6s 65ms/step - loss: 0.0672 - accuracy: 0.9813 - val_loss: 1.1912 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "91/91 [==============================] - 6s 64ms/step - loss: 0.0234 - accuracy: 0.9952 - val_loss: 1.0010 - val_accuracy: 0.4633\n",
      "Epoch 4/10\n",
      "91/91 [==============================] - 6s 65ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 1.1165 - val_accuracy: 0.3567\n",
      "Epoch 5/10\n",
      "91/91 [==============================] - 6s 65ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0154 - val_accuracy: 0.5033\n",
      "Epoch 6/10\n",
      "91/91 [==============================] - 6s 64ms/step - loss: 6.3028e-04 - accuracy: 1.0000 - val_loss: 0.9457 - val_accuracy: 0.5133\n",
      "Epoch 7/10\n",
      "91/91 [==============================] - 6s 65ms/step - loss: 6.4666e-04 - accuracy: 1.0000 - val_loss: 1.1341 - val_accuracy: 0.4733\n",
      "Epoch 8/10\n",
      "91/91 [==============================] - 6s 65ms/step - loss: 3.7205e-04 - accuracy: 1.0000 - val_loss: 1.3516 - val_accuracy: 0.3700\n",
      "Epoch 9/10\n",
      "91/91 [==============================] - 6s 66ms/step - loss: 5.2225e-04 - accuracy: 1.0000 - val_loss: 0.9398 - val_accuracy: 0.5300\n",
      "Epoch 10/10\n",
      "91/91 [==============================] - 6s 64ms/step - loss: 1.8852e-04 - accuracy: 1.0000 - val_loss: 0.9716 - val_accuracy: 0.5233\n",
      "10/10 - 0s - loss: 0.9716 - accuracy: 0.5233\n",
      "test_loss: 0.9716256856918335 \n",
      "test_accuracy: 0.5233333110809326\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "history = model.fit(x_train_reshaped, y_train,epochs=10, \n",
    "                    verbose=1, validation_data=(x_test_reshaped, y_test),\n",
    "                    shuffle=True, callbacks =[callback])\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284caaa8",
   "metadata": {},
   "source": [
    "test_accuracy가 0.52로 루브릭 기준에 못미친다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5850da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.VGG16(input_shape=(32,32,3),\n",
    "                                         include_top=False,\n",
    "                                         weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d4e28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70b73cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 14,716,227\n",
      "Trainable params: 1,539\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(3, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684b23b",
   "metadata": {},
   "source": [
    "아래와 같이 VGG16을 base_model로 학습시켰으나, 오히려 accuracy가 더 떨어졌다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a172dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "91/91 [==============================] - 48s 525ms/step - loss: 0.8622 - accuracy: 0.6192 - val_loss: 1.1937 - val_accuracy: 0.3367\n",
      "Epoch 2/5\n",
      "91/91 [==============================] - 47s 521ms/step - loss: 0.5336 - accuracy: 0.8300 - val_loss: 1.2555 - val_accuracy: 0.3400\n",
      "Epoch 3/5\n",
      "91/91 [==============================] - 47s 517ms/step - loss: 0.4166 - accuracy: 0.8750 - val_loss: 1.3641 - val_accuracy: 0.3433\n",
      "Epoch 4/5\n",
      "91/91 [==============================] - 47s 521ms/step - loss: 0.3610 - accuracy: 0.8889 - val_loss: 1.4617 - val_accuracy: 0.3400\n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 47s 516ms/step - loss: 0.3124 - accuracy: 0.9034 - val_loss: 1.4039 - val_accuracy: 0.3433\n",
      "10/10 - 4s - loss: 1.4039 - accuracy: 0.3433\n",
      "test_loss: 1.40387761592865 \n",
      "test_accuracy: 0.34333333373069763\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=5, validation_data=(x_test_reshaped, y_test))\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac64661",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "\n",
    " 가장 급박하게 한 익스였다. 결국 끝을 내지만 아쉬움이 많다. 크게 정리해보자면\n",
    " \n",
    " \n",
    " 1) data argumentation을 추가적으로 진행해볼 수 있었으나, 결국 시도해보지 못했다.\n",
    " \n",
    " 2) 데이터 prediction을 시각화하는 작업을 해볼 수 있으나 못해보았다. \n",
    " \n",
    " 3) Resnet, vgg16와 같은 모델을 transfer learning해보았으나, accuracy가 유의미하게 상승하지 않았다. 최종 제출코드에 첨부된 VGG16을 base_model로 frozen하였으나, test_accuracy가 낮게 나왔다.\n",
    " \n",
    " 4) 왜 그런가에 대해 조금 더 면밀히 탐구가 필요하다. \n",
    " \n",
    " 5) CV에 관한 익스는 처음이었는데.. Vision을 다루는 작업에 그닥 흥미를 못느끼겠다. 아마 실력이 부족해서 그런것 같다.\n",
    " \n",
    " \n",
    " 7) 제대로 안하면 회고에 멀 써야할지 잘모른다. \n",
    " \n",
    " 8) 중요한 것은 결국 루브릭 기준인 test_accuracy 60%를 못넘겼다는 사실이다. 반성하자..\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746e49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
